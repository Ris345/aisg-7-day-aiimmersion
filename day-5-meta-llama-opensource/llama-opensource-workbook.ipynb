{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama workshop notebook for IDX\n",
    "\n",
    "## Setup instructions\n",
    "\n",
    "ollama has been added in your IDX configuration \n",
    "\n",
    "\n",
    "First of all - we will need need to make sure we have the correct model downloaded\n",
    "\n",
    "(you can browse models on the official [ollama site](https://ollama.com/search))\n",
    "\n",
    "\n",
    "select a terminal - this will be serving ollama\n",
    "\n",
    "\n",
    "```bash\n",
    "ollama serve\n",
    "```\n",
    "\n",
    "now open a new terminal by clicking on the \"+\" icon\n",
    "\n",
    "```bash\n",
    "ollama pull gemma:2b\n",
    "```\n",
    "\n",
    "You can run the model by typing, you can test inference locally this way\n",
    "\n",
    "```bash\n",
    "ollama run gemma:2b\n",
    "```\n",
    "\n",
    "you can stop running a model for inference by typing\n",
    "\n",
    "```bash\n",
    "/exit\n",
    "```\n",
    "\n",
    "If you switch back to the terminal where ollama serve is running - you can see the API interactions/logs\n",
    "\n",
    "\n",
    "Now, make sure the kernel for this notebook is your lab_env virtual environment\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install requirements\n",
    "\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import logging\n",
    "from llama_index.core import SimpleDirectoryReader, Settings, VectorStoreIndex\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.llms.ollama import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple logging - so we can see some of the details printed out\n",
    "\n",
    "\n",
    "def set_logging():\n",
    "    # Create a logger instance\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.DEBUG)  # Set the minimum logging level for the logger\n",
    "\n",
    "    # Create a stream handler that outputs to the console\n",
    "    stream_handler = logging.StreamHandler()\n",
    "    stream_handler.setLevel(logging.INFO)  # Set the handler's level to INFO or DEBUG\n",
    "\n",
    "    # Create a formatter to customize log output (optional, but recommended)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    stream_handler.setFormatter(formatter)\n",
    "\n",
    "    # Add the stream handler to the logger\n",
    "    logger.addHandler(stream_handler)\n",
    "\n",
    "\n",
    "\n",
    "    #set up the global logger to have the same level and handler\n",
    "    logging.basicConfig(level=logging.INFO, handlers=[stream_handler])\n",
    "\n",
    "set_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-02 13:36:24,737 - root - INFO - Loading documents...\n",
      "2025-01-02 13:36:25,298 - root - INFO - Loaded 5 documents.\n"
     ]
    }
   ],
   "source": [
    "# 1. Load your data\n",
    "logging.info(\"Loading documents...\")\n",
    "documents = SimpleDirectoryReader(input_dir=\"ollama-documents/\").load_data()\n",
    "logging.info(f\"Loaded {len(documents)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-02 13:36:36,542 - root - INFO - Setting up Ollama LLM...\n",
      "2025-01-02 13:36:36,545 - root - INFO - Setting up Ollama Embedding...\n"
     ]
    }
   ],
   "source": [
    "# 2. Setup Ollama LLM\n",
    "# we want the embeddings to synthesize document info!\n",
    "logging.info(\"Setting up Ollama LLM...\")\n",
    "Settings.llm = Ollama(model=\"gemma:2b\", request_timeout=120.0)\n",
    "logging.info(\"Setting up Ollama Embedding...\")\n",
    "\n",
    "ollama_embedding = OllamaEmbedding(\n",
    "    model_name=\"gemma:2b\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    ollama_additional_kwargs={\"mirostat\": 0, \"request_timeout\": 120},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-02 13:36:40,740 - root - INFO - Creating index...\n",
      "2025-01-02 13:36:51,784 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-01-02 13:37:09,139 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-01-02 13:37:30,150 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-01-02 13:37:49,756 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-01-02 13:38:08,871 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-01-02 13:38:08,880 - root - INFO - Setting up index completed...\n"
     ]
    }
   ],
   "source": [
    "# 3. Create the index\n",
    "\n",
    "# This can take some time\n",
    "logging.info(\"Creating index...\")\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=ollama_embedding)\n",
    "logging.info(\"Setting up index completed...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-02 13:39:07,631 - root - INFO - Querying index...\n",
      "2025-01-02 13:39:08,285 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-01-02 13:40:17,490 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The context does not provide information about Scarecrow, so I cannot answer this query from the provided context.\n"
     ]
    }
   ],
   "source": [
    "# 4. Query the index\n",
    "\n",
    "#This will take more than a minute as well on IDX\n",
    "logging.info(\"Querying index...\")\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "sitrep = query_engine.query(\"Scarecrow?\")\n",
    "print(sitrep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
