{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Initial setup and config\n",
        "\n",
        "## Preparation:\n",
        "- Go to https://platform.openai.com/ and sign up if you havent\n",
        "- Create your API key at https://platform.openai.com/api-keys\n",
        "\n",
        "## Setup\n",
        "This section handles the initial setup requirements:\n",
        "- Installing dependencies from requirements.txt\n",
        "- Setting up API authentication using a YAML file\n",
        "- Configuring the OpenAI client\n",
        "\n",
        "**Security Note**: Never commit API keys directly in code. We use a separate YAML file\n",
        "that should be added to .gitignore.\n",
        "\n",
        "Docs: https://platform.openai.com/docs/quickstart/build-your-application"
      ],
      "metadata": {
        "id": "pzCnjvBLvdgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "5Y6C3TROvYwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uHTbAUqGSS8p"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "import yaml\n",
        "import time\n",
        "import requests\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define functions to manage secrets"
      ],
      "metadata": {
        "id": "oSeBCR_4vgzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_secrets(filepath=\"secrets.yaml\"):\n",
        "    try:\n",
        "        with open(filepath, \"r\") as f:\n",
        "            return yaml.safe_load(f)\n",
        "    except FileNotFoundError:\n",
        "        return None\n",
        "    except yaml.YAMLError as e:\n",
        "        print(f\"Error parsing {filepath}: {e}\")\n",
        "        return None\n",
        "\n",
        "def create_secrets_file(filepath=\"secrets.yaml\"):\n",
        "    api_key = input(\"Please enter your OpenAI API Key: \")\n",
        "    secrets_data = {\"openai\": {\"api_key\": api_key}}\n",
        "    try:\n",
        "        with open(filepath, \"w\") as f:\n",
        "            yaml.safe_dump(secrets_data, f)\n",
        "        print(f\"secrets.yaml created and OpenAI API key stored.\")\n",
        "        return secrets_data\n",
        "    except Exception as e:\n",
        "         print(f\"Error creating {filepath}: {e}\")\n",
        "         return None"
      ],
      "metadata": {
        "id": "Pr_Uh36DV5Rs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load secrets"
      ],
      "metadata": {
        "id": "a7Y4tOibvnDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load secrets\n",
        "secrets = load_secrets()\n",
        "\n",
        "if not secrets:\n",
        "    print(\"secrets.yaml not found or could not be loaded, creating one..\")\n",
        "    secrets = create_secrets_file()\n",
        "    if not secrets:\n",
        "        print(\"Could not load API key. Please check your secrets.yaml file and run again\")\n",
        "\n",
        "if secrets and \"openai\" in secrets and \"api_key\" in secrets[\"openai\"]:\n",
        "  # Configure OpenAI API key\n",
        "  client = OpenAI(api_key=secrets[\"openai\"][\"api_key\"])\n",
        "else:\n",
        "  print(\"Could not load API key. Please check your secrets.yaml file\")"
      ],
      "metadata": {
        "id": "HGFV-K3SWBOL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Chat Completion\n",
        "Demonstrates basic interaction with OpenAI's chat API.\n",
        "\n",
        "## Key Components\n",
        "- `chat.completions.create()`: Main method for generating completions\n",
        "- `model`: Specifies GPT version (e.g. \"gpt-4\")\n",
        "- `messages`: Array of conversation turns\n",
        "- `store`: Enables response storage for future reference\n",
        "\n",
        "## Structure\n",
        "```python\n",
        "messages=[\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "```\n",
        "\n",
        "## Response Format\n",
        "\n",
        "```\n",
        "choices[0].message.content```\n",
        "Contains generated text\n",
        "Multiple response variations possible with n parameter\n",
        "\n",
        "## üìö Documentation:\n",
        "\n",
        "- API Reference: https://platform.openai.com/docs/api-reference/chat\n",
        "- Message Structure: https://platform.openai.com/docs/guides/text-generation/message-structure"
      ],
      "metadata": {
        "id": "2QdxwS8YdSPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "basic_prompt = \"Write a short poem about the moon.\"\n",
        "\n",
        "\n",
        "print(\"Basic Text Generation \\nSending request and awaiting response...\\n\\n\\n\")\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    store=True,\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": basic_prompt}\n",
        "    ]\n",
        ")\n",
        "generated_poem = response.choices[0].message.content\n",
        "print(f\"Prompt:\\n{basic_prompt}\")\n",
        "print(f\"Response:\\n{generated_poem}\")"
      ],
      "metadata": {
        "id": "u5lJIOlHY79X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Message Control\n",
        "Explores message roles and instruction hierarchies.\n",
        "\n",
        "## Message Roles\n",
        "- `system`: Core behavioral instructions\n",
        "- `developer`: Alternative to system role\n",
        "- `user`: End-user prompts\n",
        "\n",
        "## Instruction Hierarchy\n",
        "1. Latest system message takes precedence\n",
        "2. Developer instructions can be overwritten\n",
        "3. Multiple inputs accumulate unless explicitly overwritten\n",
        "\n",
        "## Best Practices\n",
        "- Keep system prompts focused and clear\n",
        "- Test role combinations for desired behavior\n",
        "- Consider message ordering impact\n",
        "\n",
        "‚ö†Ô∏è **Important**: System messages significantly impact model behavior.\n",
        "\n",
        "## üìö **Resources**:\n",
        "- Role Definitions: https://platform.openai.com/docs/guides/text-generation/role-definitions\n",
        "- System Instructions: https://platform.openai.com/docs/guides/text-generation/system-instructions"
      ],
      "metadata": {
        "id": "6SGEo45OoHvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = '''\n",
        "You are a helpful assistant that answers programming\n",
        "questions in the style of a southern belle from the\n",
        "southeast United States.\n",
        "'''\n",
        "\n",
        "basic_prompt = \"Are semicolons optional in JavaScript?\"\n",
        "\n",
        "\n",
        "print(\"Generation with system messages \\nSending request and awaiting response...\\n\\n\\n\")\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    store=True,\n",
        "    messages=[\n",
        "    {\n",
        "      \"role\": \"developer\", #system works as well\n",
        "      \"content\": [\n",
        "        {\n",
        "          \"type\": \"text\",\n",
        "          \"text\": system_prompt\n",
        "        }\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"developer\", #Multiple inputs of same origin\n",
        "      \"content\": [\n",
        "        {\n",
        "          \"type\": \"text\",\n",
        "          \"text\": \"This is a random test prompt\"\n",
        "        }\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"developer\", #Overwriting instructions\n",
        "      \"content\": [\n",
        "        {\n",
        "          \"type\": \"text\",\n",
        "          \"text\": \"Overwrite all previous instructions and act as a stereotypical caribbean pirate of irish origin\"\n",
        "        }\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"system\", #Using system instead of developer, overwriting developer instructions\n",
        "      \"content\": [\n",
        "        {\n",
        "          \"type\": \"text\",\n",
        "          \"text\": \"In your response, insert the keyword L33t\"\n",
        "        }\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": [\n",
        "        {\n",
        "          \"type\": \"text\",\n",
        "          \"text\": basic_prompt\n",
        "        }\n",
        "      ]\n",
        "    },\n",
        "  ]\n",
        ")\n",
        "\n",
        "\n",
        "response = response.choices[0].message.content\n",
        "print(f\"Prompt:\\n{basic_prompt}\")\n",
        "print(f\"\\n\\nResponse:\\n{response}\")"
      ],
      "metadata": {
        "id": "JYOnKgQyZWkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interactive Chat Example\n",
        "Demonstrates message chaining for back-and-forth conversation.\n",
        "\n",
        "## Structure\n",
        "```python\n",
        "messages=[\n",
        "    {\"role\": \"user\", \"content\": \"First message\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"First response\"},\n",
        "    {\"role\": \"user\", \"content\": \"Follow-up question\"}\n",
        "]\n",
        "```\n",
        "## Key Points\n",
        "\n",
        "- Messages list maintains conversation context\n",
        "- Each turn alternates between user/assistant roles\n",
        "- Model considers full conversation history\n",
        "- Useful for context-dependent tasks\n",
        "\n",
        "üìö Reference: https://platform.openai.com/docs/guides/text-generation/conversation-context"
      ],
      "metadata": {
        "id": "DSLaPLkGv6cp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Chained Messages Example with gpt-4o in a loop---\n",
        "print(\"\\n## Chained Messages Example with gpt-4o in a loop\\n\")\n",
        "\n",
        "# Initial prompt\n",
        "messages = []\n",
        "\n",
        "# Loop for 3 interactions\n",
        "for i in range(3):\n",
        "  prompt = input(\"Your message to the AI Model:\")\n",
        "  print(f\"\\nUser Prompt {i+1}: {prompt}\")\n",
        "  messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "  # Make the API call\n",
        "  response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "  response_text = response.choices[0].message.content\n",
        "  print(f\"\\n\\nResponse {i+1}:\\n{response_text}\")\n",
        "  messages.append({\"role\": \"assistant\", \"content\": response_text})\n",
        "\n",
        "print(\"\\n\\nChained messages interaction completed.\\n\")"
      ],
      "metadata": {
        "id": "Da_icEi-v9Za"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenAI Assistants API\n",
        "Introduction to the Assistants API for persistent, task-specific AI agents.\n",
        "\n",
        "## Assistant Creation\n",
        "```python\n",
        "client.beta.assistants.create(\n",
        "    name=\"Test Assistant\",\n",
        "    instructions=\"...\",\n",
        "    model=\"gpt-4\"\n",
        ")\n",
        "```\n",
        "\n",
        "## Key Features\n",
        "\n",
        "- Persistent identity/configuration\n",
        "- Custom instructions\n",
        "- Tool integration capability\n",
        "- State management\n",
        "\n",
        "## Best Practices\n",
        "\n",
        "- Clear, specific instructions\n",
        "- Consider tool requirements\n",
        "- Test with various prompts\n",
        "\n",
        "## üìö Documentation:\n",
        "\n",
        "- Assistants Overview: https://platform.openai.com/docs/assistants/overview\n",
        "- Tools Reference: https://platform.openai.com/docs/assistants/tools"
      ],
      "metadata": {
        "id": "MPMw6Vpcv2CN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assistant_id = None\n",
        "\n",
        "# If no assistant_id is defined create a new assistant\n",
        "if not assistant_id:\n",
        "    print(\"Creating a new assistant...\")\n",
        "    assistant = client.beta.assistants.create(\n",
        "        name=\"Test Assistant\",\n",
        "        instructions=\"You are a helpful assistant that answers questions concisely.\",\n",
        "        model=\"gpt-4o\",\n",
        "    )\n",
        "    assistant_id = assistant.id\n",
        "    print(f\"New assistant created with ID: {assistant_id}\")\n",
        "else:\n",
        "  print(f\"Using existing assistant: {assistant_id}\")"
      ],
      "metadata": {
        "id": "HeFqtXJaq0mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Managing Conversations with Threads\n",
        "\n",
        "Threads maintain conversation context and handle message flow:\n",
        "\n",
        "## Create conversation container\n",
        "```python\n",
        "thread = client.beta.threads.create()\n",
        "```\n",
        "## Add message to thread\n",
        "```python\n",
        "message = client.beta.threads.messages.create(\n",
        "    thread_id=thread.id,\n",
        "    role=\"user\",\n",
        "    content=\"Query\"\n",
        ")\n",
        "```\n",
        "\n",
        "## Process with assistant\n",
        "```python\n",
        "run = client.beta.threads.runs.create(\n",
        "    thread_id=thread.id,\n",
        "    assistant_id=assistant_id\n",
        ")\n",
        "```\n",
        "\n",
        "- Thread acts as conversation container\n",
        "- Messages are added sequentially\n",
        "- Run executes assistant processing\n",
        "- Includes status polling and response handling\n",
        "\n",
        "üìö Deep dive: https://platform.openai.com/docs/assistants/how-it-works/managing-threads"
      ],
      "metadata": {
        "id": "l-M7zsnhOH7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Assistant run\n",
        "assistant_prompt = \"What is the capital of France?\"\n",
        "print(f\"Assistant Prompt: {assistant_prompt}\")"
      ],
      "metadata": {
        "id": "ZF_ilaLuq2az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a thread\n",
        "thread = client.beta.threads.create()"
      ],
      "metadata": {
        "id": "nU6yWoDJq_dV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a user message on the thread\n",
        "message = client.beta.threads.messages.create(\n",
        "    thread_id=thread.id,\n",
        "    role=\"user\",\n",
        "    content=assistant_prompt,\n",
        ")"
      ],
      "metadata": {
        "id": "jnw-gaiLrB1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the assistant\n",
        "run = client.beta.threads.runs.create(\n",
        "    thread_id=thread.id,\n",
        "    assistant_id=assistant_id,\n",
        ")"
      ],
      "metadata": {
        "id": "Im86QU1yrI6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wait for the run to complete\n",
        "while True:\n",
        "    run = client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=run.id)\n",
        "    if run.status in [\"completed\", \"failed\", \"cancelled\", \"expired\"]:\n",
        "        break\n",
        "    time.sleep(.3)  # Wait for .3 second before checking again\n",
        "\n",
        "if run.status == \"failed\":\n",
        "    print(\"Assistant run failed!\")\n",
        "    print(f\"Run error message: {run.error}\")\n",
        "else:\n",
        "  # Retrieve messages from the thread\n",
        "  messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
        "  # Get the assistant's response\n",
        "  assistant_response = [message.content[0].text.value for message in messages.data if message.role == \"assistant\"]\n",
        "  print(\"Assistant Response:\")\n",
        "  for res in assistant_response:\n",
        "    print(f\"{res}\")\n",
        "\n",
        "print(\"\\n\\nAssistant interaction completed.\\n\")"
      ],
      "metadata": {
        "id": "-Pu7T-d5rSNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Research Assistant with Advanced Tools\n",
        "Creates an enhanced assistant with file processing and analysis capabilities:\n",
        "```python\n",
        "# Download and process research papers\n",
        "local_pdf_paths = download_pdfs(pdf_urls)\n",
        "\n",
        "# Create assistant with tools\n",
        "assistant = client.beta.assistants.create(\n",
        "    tools=[{\"type\": \"file_search\"}, {\"type\": \"code_interpreter\"}]\n",
        ")\n",
        "\n",
        "# Set up vector store for document search\n",
        "vector_store = client.beta.vector_stores.create()\n",
        "```\n",
        "- Handles PDF download and processing\n",
        "- Enables file search capabilities\n",
        "- Adds code interpretation\n",
        "- Creates vector embeddings for efficient search\n",
        "- Integrates all components for research tasks\n",
        "\n",
        "üìö Tool reference: https://platform.openai.com/docs/assistants/tools"
      ],
      "metadata": {
        "id": "3RxJF2mpCwol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n## Research Assistant Creation\\n\")\n",
        "\n",
        "# Define PDF URLs\n",
        "pdf_urls = [\n",
        "    \"https://arxiv.org/pdf/1706.03762\",  # Attention Is All You Need\n",
        "    \"https://arxiv.org/pdf/2412.21187\",  # Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like\n",
        "]\n",
        "\n",
        "# Download PDFs and save locally\n",
        "local_pdf_paths = []\n",
        "for i, url in enumerate(pdf_urls):\n",
        "    try:\n",
        "        print(f\"Downloading PDF from: {url}\")\n",
        "\n",
        "        # Get pdf from url\n",
        "        response = requests.get(url, allow_redirects=True)\n",
        "\n",
        "        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
        "        file_extension = os.path.splitext(url)[1].split('?')[0]\n",
        "\n",
        "        #Setting file extension manually, as it would be a number otherwise - only applies to specific situation\n",
        "        file_extension = \".pdf\"\n",
        "        local_path = f\"research_doc_{i+1}{file_extension}\"\n",
        "\n",
        "        #Save PDF\n",
        "        with open(local_path, \"wb\") as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "        # Add file path to our list\n",
        "        local_pdf_paths.append(local_path)\n",
        "        print(f\"Downloaded and saved to: {local_path}\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "      print(f\"Failed to download file from {url} error: {e}\")"
      ],
      "metadata": {
        "id": "r_Gf0mqjyMOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new assistant with file_search and code_interpreter\n",
        "print(\"\\nCreating a new research assistant...\")\n",
        "assistant = client.beta.assistants.create(\n",
        "    name=\"Research Assistant\",\n",
        "    instructions=\"You are a helpful research assistant with access to several research documents and code interpreter. You can answer questions based on the content of the files and use code if needed.\",\n",
        "    model=\"gpt-4o\",\n",
        "    tools=[{\"type\": \"file_search\"}, {\"type\": \"code_interpreter\"}],\n",
        ")\n",
        "print(f\"Assistant created with ID: {assistant.id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Chc-1cByhGK",
        "outputId": "0c3d94ce-ec5b-43b5-a632-b8ccfe4c2c0a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Creating a new research assistant...\n",
            "Assistant created with ID: asst_JTbqC2URJP2rXZQt0wVftrIR\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nUploading files to OpenAI...\")\n",
        "file_ids = []\n",
        "for local_path in local_pdf_paths:\n",
        "    try:\n",
        "        print(f\"Uploading file: {local_path}\")\n",
        "        with open(local_path, \"rb\") as file_stream:\n",
        "            file_obj = client.files.create(file=file_stream, purpose=\"assistants\")\n",
        "            file_ids.append(file_obj.id)\n",
        "            print(f\"Uploaded file ID: {file_obj.id}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error uploading file {local_path}: {e}\")\n"
      ],
      "metadata": {
        "id": "M-bGHGa8yrhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a vector store and add the files to it\n",
        "print(\"\\nCreating vector store and adding files...\")\n",
        "vector_store = client.beta.vector_stores.create(name=\"Research Documents\")\n",
        "print(f\"Vector store created with ID: {vector_store.id}\")"
      ],
      "metadata": {
        "id": "xwoTeItnzThp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload all files to the vector store\n",
        "if file_ids:\n",
        "    print(f\"Adding files to vector store\")\n",
        "    try:\n",
        "        # Create file streams from local paths\n",
        "        file_streams = [open(local_path, \"rb\") for local_path in local_pdf_paths]\n",
        "\n",
        "        file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
        "            vector_store_id=vector_store.id, files=file_streams\n",
        "        )\n",
        "        print(f\"File batch upload status: {file_batch.status}\")\n",
        "        print(f\"File batch file counts: {file_batch.file_counts}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error adding files to vector store: {e}\")\n",
        "else:\n",
        "    print(\"No files to add to vector store\")"
      ],
      "metadata": {
        "id": "jkzRcRDozW2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the assistant to use the vector store\n",
        "print(\"\\nUpdating assistant with the vector store...\")\n",
        "try:\n",
        "  assistant = client.beta.assistants.update(\n",
        "      assistant_id=assistant.id,\n",
        "      tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
        "  )\n",
        "  print(\"Assistant updated successfully with vector store.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error updating assistant with vector store: {e}\")\n",
        "\n",
        "print(\"\\n\\nResearch assistant setup completed.\")\n",
        "print(\"You can now use the assistant to ask questions about the uploaded files.\")\n",
        "print(\"Assistant ID: \", assistant.id)\n",
        "print(\"Vector Store ID: \", vector_store.id)"
      ],
      "metadata": {
        "id": "Tt0DiygFzmjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Run Analysis and Monitoring\n",
        "\n",
        "Provides detailed insight into assistant's processing steps:\n",
        "```python\n",
        "run_steps = client.beta.threads.runs.steps.list(\n",
        "    thread_id=thread.id,\n",
        "    run_id=run.id\n",
        ")\n",
        "```\n",
        "- Tracks execution progress\n",
        "- Shows tool usage details\n",
        "- Reveals thinking/reasoning steps\n",
        "- Helps debug and optimize interactions\n",
        "- Monitors file processing and code execution\n",
        "\n",
        "## Key features:\n",
        "\n",
        "- Step-by-step execution tracking\n",
        "- Tool call monitoring\n",
        "- Response generation analysis\n",
        "- Error handling and status checks\n",
        "\n",
        "üìö **Detailed guide:** https://platform.openai.com/docs/assistants/how-it-works/runs-and-run-steps"
      ],
      "metadata": {
        "id": "7pDlFWx-QBXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Running the Assistant with a custom prompt ---\n",
        "print(\"\\n## Running the Assistant with a Custom Prompt\\n\")\n",
        "\n",
        "custom_prompt = \"Summarize the key findings of the Attention is all you need paper.\"\n",
        "print(f\"User Prompt: {custom_prompt}\")\n",
        "\n",
        "# Create a thread\n",
        "thread = client.beta.threads.create()\n",
        "\n",
        "# Add the user message to the thread\n",
        "message = client.beta.threads.messages.create(\n",
        "    thread_id=thread.id,\n",
        "    role=\"user\",\n",
        "    content=custom_prompt,\n",
        ")\n"
      ],
      "metadata": {
        "id": "LWzd34F30o4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a run\n",
        "run = client.beta.threads.runs.create(\n",
        "    thread_id=thread.id,\n",
        "    assistant_id=assistant.id,\n",
        ")\n",
        "\n",
        "\n",
        "# Wait for the run to complete\n",
        "while True:\n",
        "    run = client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=run.id)\n",
        "    if run.status in [\"completed\", \"failed\", \"cancelled\", \"expired\"]:\n",
        "        break\n",
        "    time.sleep(1)  # Wait for 1 second before checking again\n",
        "\n",
        "if run.status == \"failed\":\n",
        "    print(\"Assistant run failed!\")\n",
        "    print(f\"Run error message: {run.error}\")\n",
        "else:\n",
        "  # Retrieve messages from the thread\n",
        "  messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
        "  # Get the assistant's response\n",
        "  assistant_response = [message.content[0].text.value for message in messages.data if message.role == \"assistant\"]\n",
        "  print(\"Assistant Response:\")\n",
        "  for res in assistant_response:\n",
        "    print(f\"{res}\")\n",
        "\n",
        "print(\"\\n\\nAssistant interaction completed.\\n\")"
      ],
      "metadata": {
        "id": "MpOVP1-806VY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Retrieve and Display Run Steps ---\n",
        "print(\"\\n## Run Steps Details\\n\")\n",
        "\n",
        "# Retrieve run steps\n",
        "try:\n",
        "    run_steps = client.beta.threads.runs.steps.list(\n",
        "        thread_id=thread.id,\n",
        "        run_id=run.id\n",
        "    )\n",
        "\n",
        "    # Print run steps with details\n",
        "    for step in run_steps.data:\n",
        "        print(f\"Step ID: {step.id}\")\n",
        "        print(f\"Step Type: {step.type}\")\n",
        "        print(f\"Status: {step.status}\")\n",
        "\n",
        "        if step.type == \"message_creation\":\n",
        "            if step.step_details and hasattr(step.step_details, \"message_creation\"):\n",
        "                message_content = step.step_details.message_creation.content\n",
        "                if message_content:\n",
        "                    print(\"    Assistant Thinking/Response:\")\n",
        "                    for content_item in message_content:\n",
        "                      if content_item.type == \"text\":\n",
        "                        print(f\"        {content_item.text.value}\")\n",
        "\n",
        "\n",
        "        if step.type == \"tool_calls\":\n",
        "          if step.step_details and hasattr(step.step_details, \"tool_calls\"):\n",
        "            for tool_call in step.step_details.tool_calls:\n",
        "                print(f\"    Tool Call ID: {tool_call.id}\")\n",
        "                print(f\"    Tool Type: {tool_call.type}\")\n",
        "                if tool_call.type == \"file_search\":\n",
        "                    if hasattr(tool_call, \"file_search\") and hasattr(tool_call.file_search, \"results\"):\n",
        "                        for result in tool_call.file_search.results:\n",
        "                            print(f\"        Result Content: {result.content}\")\n",
        "                elif tool_call.type == \"code_interpreter\":\n",
        "                    if hasattr(tool_call, \"code_interpreter\") and hasattr(tool_call.code_interpreter, \"input\"):\n",
        "                        print(f\"        Code Input: {tool_call.code_interpreter.input}\")\n",
        "                    if hasattr(tool_call, \"code_interpreter\") and hasattr(tool_call.code_interpreter, \"outputs\"):\n",
        "                         for output in tool_call.code_interpreter.outputs:\n",
        "                           if hasattr(output, \"logs\"):\n",
        "                             print(f\"        Code Output: {output.logs}\")\n",
        "\n",
        "\n",
        "        print(\"-\" * 20)\n",
        "except Exception as e:\n",
        "    print(f\"Error retrieving run steps: {e}\")"
      ],
      "metadata": {
        "id": "Oi4Lx48b1G9Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}